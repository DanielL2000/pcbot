{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               object\n",
      "comment_text     object\n",
      "toxic             int64\n",
      "severe_toxic      int64\n",
      "obscene           int64\n",
      "threat            int64\n",
      "insult            int64\n",
      "identity_hate     int64\n",
      "dtype: object\n",
      "Toxic: 0.0958444830201\n"
     ]
    }
   ],
   "source": [
    "# LTSM->CNN Model implementation\n",
    "# Inspired by: http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # Handle csv data and dataframes\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim # Load Pre-trained word2vec embeddings\n",
    "\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('../../data/train.csv')\n",
    "test_df  = pd.read_csv('../../data/test.csv')\n",
    "print(train_df.dtypes)\n",
    "\n",
    "# Identify % of data that is toxic\n",
    "list_class = ['toxic', 'severe_toxic',\n",
    "              'obscene', 'threat', \n",
    "              'insult', 'identity_hate'\n",
    "]\n",
    "p_toxic = sum(train_df['toxic'])/float(len(train_df))\n",
    "\n",
    "print(\"Toxic\" + \": \" + str(p_toxic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Training & Evaluation Inputs\n",
    "y_eval = train_df[list_class].values\n",
    "\n",
    "sentences_train = train_df['comment_text']\n",
    "sentences_test = test_df['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize texts\n",
    "max_features = 300000\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "tokenized_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "tokenized_test  = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "# Pad tokenized sequences\n",
    "maxlen = 200\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "X_test  = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary from pre-trained Google Word2Vec Embeddings\n",
    "embeddings_index = {}\n",
    "path_to_wv = '/Volumes/bluelight/word2vec/GoogleNews-vectors-negative300.txt'\n",
    "f = open(path_to_wv)\n",
    "\n",
    "for line in f:\n",
    "    embedding = line.split()\n",
    "    word = embedding[0]\n",
    "    vec = np.asarray(embedding[1:], dtype='float32')\n",
    "    embeddings_index[word] = vec \n",
    "    \n",
    "embedding_dim=300 # customize embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Embedding Matrix from dict\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 200, 300)     63166500    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 200, 300)     0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 200, 256)     439296      spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 198, 64)      49216       bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          16512       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            774         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 63,672,298\n",
      "Trainable params: 505,798\n",
      "Non-trainable params: 63,166,500\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen, ))\n",
    "x = Embedding(embedding_matrix.shape[0],\n",
    "              embedding_matrix.shape[1],\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=200,\n",
    "              trainable=False)(inp)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "x = concatenate([avg_pool, max_pool]) \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "preds = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inp, preds)\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143613/143613 [==============================] - 2349s 16ms/step - loss: 0.0650 - acc: 0.9780 - val_loss: 0.0527 - val_acc: 0.9808\n",
      "Epoch 2/4\n",
      "143613/143613 [==============================] - 2431s 17ms/step - loss: 0.0489 - acc: 0.9819 - val_loss: 0.0482 - val_acc: 0.9821\n",
      "Epoch 3/4\n",
      "143613/143613 [==============================] - 2141s 15ms/step - loss: 0.0456 - acc: 0.9828 - val_loss: 0.0487 - val_acc: 0.9827\n",
      "Epoch 4/4\n",
      "143613/143613 [==============================] - 2436s 17ms/step - loss: 0.0433 - acc: 0.9834 - val_loss: 0.0473 - val_acc: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e9cab50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 4\n",
    "model.fit(X_train,y_eval, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, \n",
    "          validation_split=0.1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../trained_model/lstm_cnn_w2v/pc_net.h5')\n",
    "\n",
    "# Save Tokenizer\n",
    "import pickle\n",
    "with open('../../trained_model/lstm_cnn_w2v/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
